{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jiachen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jiachen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_tweet</th>\n",
       "      <th>main_tweet_id</th>\n",
       "      <th>verified</th>\n",
       "      <th>followers</th>\n",
       "      <th>replies</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 regularly rinsing nose saline help prevent i...</td>\n",
       "      <td>1250219300389974016</td>\n",
       "      <td>False</td>\n",
       "      <td>410</td>\n",
       "      <td>[{\"tweet_id\": 1250219116993974272, \"tweet\": \"4...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french police chief killed charliehebdo attack</td>\n",
       "      <td>554886875303780352</td>\n",
       "      <td>True</td>\n",
       "      <td>3229894</td>\n",
       "      <td>[{\"tweet_id\": 554959644125167617, \"tweet\": \"De...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus disease covid19 advice public\\r\\r‚ú≥...</td>\n",
       "      <td>1237901309011021825</td>\n",
       "      <td>False</td>\n",
       "      <td>613</td>\n",
       "      <td>[{\"tweet_id\": 1237901311439450112, \"tweet\": \"I...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ottawa police confirm multiple suspects shooti...</td>\n",
       "      <td>524958128392376320</td>\n",
       "      <td>True</td>\n",
       "      <td>19783124</td>\n",
       "      <td>[{\"tweet_id\": 524961934064754688, \"tweet\": \"@W...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>primary focus government isnt alleviate suffer...</td>\n",
       "      <td>1239295488677085185</td>\n",
       "      <td>False</td>\n",
       "      <td>4889</td>\n",
       "      <td>[]</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>4 cannot transmitted goods manufactured china ...</td>\n",
       "      <td>1237545128828342277</td>\n",
       "      <td>False</td>\n",
       "      <td>631</td>\n",
       "      <td>[{\"tweet_id\": 1237545126278258703, \"tweet\": \"#...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>desperate ted cruz claims planned parenthood s...</td>\n",
       "      <td>671181758692507648</td>\n",
       "      <td>True</td>\n",
       "      <td>143090</td>\n",
       "      <td>[{\"tweet_id\": 671200376843067392, \"tweet\": \"@B...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>thoughts prayers enough pres obama speaks mass...</td>\n",
       "      <td>672513234419638273</td>\n",
       "      <td>True</td>\n",
       "      <td>17449031</td>\n",
       "      <td>[{\"tweet_id\": 672513853645717504, \"tweet\": \"@A...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>police surrounded building suspected charliehe...</td>\n",
       "      <td>553508098825261056</td>\n",
       "      <td>True</td>\n",
       "      <td>9077962</td>\n",
       "      <td>[{\"tweet_id\": 553509546602553344, \"tweet\": \"@N...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>lynnesimpkin help üë©‚Äçüè´\\r9am socialism  need bad...</td>\n",
       "      <td>1241082793737818113</td>\n",
       "      <td>False</td>\n",
       "      <td>3103</td>\n",
       "      <td>[{\"tweet_id\": 1241041450084839424, \"tweet\": \"E...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             main_tweet        main_tweet_id  \\\n",
       "0     5 regularly rinsing nose saline help prevent i...  1250219300389974016   \n",
       "1      french police chief killed charliehebdo attack     554886875303780352   \n",
       "2     coronavirus disease covid19 advice public\\r\\r‚ú≥...  1237901309011021825   \n",
       "3     ottawa police confirm multiple suspects shooti...   524958128392376320   \n",
       "4     primary focus government isnt alleviate suffer...  1239295488677085185   \n",
       "...                                                 ...                  ...   \n",
       "1889  4 cannot transmitted goods manufactured china ...  1237545128828342277   \n",
       "1890  desperate ted cruz claims planned parenthood s...   671181758692507648   \n",
       "1891  thoughts prayers enough pres obama speaks mass...   672513234419638273   \n",
       "1892  police surrounded building suspected charliehe...   553508098825261056   \n",
       "1894  lynnesimpkin help üë©‚Äçüè´\\r9am socialism  need bad...  1241082793737818113   \n",
       "\n",
       "      verified  followers                                            replies  \\\n",
       "0        False        410  [{\"tweet_id\": 1250219116993974272, \"tweet\": \"4...   \n",
       "1         True    3229894  [{\"tweet_id\": 554959644125167617, \"tweet\": \"De...   \n",
       "2        False        613  [{\"tweet_id\": 1237901311439450112, \"tweet\": \"I...   \n",
       "3         True   19783124  [{\"tweet_id\": 524961934064754688, \"tweet\": \"@W...   \n",
       "4        False       4889                                                 []   \n",
       "...        ...        ...                                                ...   \n",
       "1889     False        631  [{\"tweet_id\": 1237545126278258703, \"tweet\": \"#...   \n",
       "1890      True     143090  [{\"tweet_id\": 671200376843067392, \"tweet\": \"@B...   \n",
       "1891      True   17449031  [{\"tweet_id\": 672513853645717504, \"tweet\": \"@A...   \n",
       "1892      True    9077962  [{\"tweet_id\": 553509546602553344, \"tweet\": \"@N...   \n",
       "1894     False       3103  [{\"tweet_id\": 1241041450084839424, \"tweet\": \"E...   \n",
       "\n",
       "          label  \n",
       "0     nonrumour  \n",
       "1        rumour  \n",
       "2     nonrumour  \n",
       "3     nonrumour  \n",
       "4     nonrumour  \n",
       "...         ...  \n",
       "1889  nonrumour  \n",
       "1890     rumour  \n",
       "1891     rumour  \n",
       "1892  nonrumour  \n",
       "1894  nonrumour  \n",
       "\n",
       "[1567 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_file = \"train.data.txt\"\n",
    "dev_file = \"dev.data.txt\"\n",
    "test_file = \"test.data.txt\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def tokenize_tweet(string_data:str):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenized = nltk.RegexpTokenizer('\\w+')\n",
    "    data = string_data.replace('\\n', '')\n",
    "    data = data.lower()\n",
    "    data = re.sub('https?://\\S+|www\\.\\S+', '', data)\n",
    "    data = re.sub('[%s]' % re.escape(string.punctuation), '', data)\n",
    "    # data = [wordnet_lemmatizer.lemmatize(word) for word in data.split(' ')]\n",
    "    data = ' '.join([i for i in data.split(' ') if i not in stopwords])\n",
    "    return data\n",
    "\n",
    "train_data = pd.read_csv('./%s.csv'%train_file,keep_default_na=False)\n",
    "dev_data = pd.read_csv('./%s.csv'%dev_file,keep_default_na=False)\n",
    "test_data = pd.read_csv('./%s.csv'%test_file,keep_default_na=False)\n",
    "def preprocess_token(df, dropNa=True):\n",
    "    data = df.copy()\n",
    "    data['main_tweet'] = data['main_tweet'].fillna('')\n",
    "    if dropNa:\n",
    "        data.replace('', np.nan, inplace=True)\n",
    "        data.dropna(subset=['main_tweet'], inplace=True)\n",
    "    text = data['main_tweet'].apply(lambda x: tokenize_tweet(x))\n",
    "    data['main_tweet'] = text\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data=preprocess_token(train_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (4.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\jiachen\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1567, 100)\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "49/49 [==============================] - 45s 523ms/step - loss: 0.5118 - accuracy: 0.7403 - val_loss: 0.5075 - val_accuracy: 0.8523\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 22s 447ms/step - loss: 0.2417 - accuracy: 0.9138 - val_loss: 0.3863 - val_accuracy: 0.9084\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 22s 453ms/step - loss: 0.1417 - accuracy: 0.9636 - val_loss: 0.2889 - val_accuracy: 0.9271\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 22s 450ms/step - loss: 0.0674 - accuracy: 0.9879 - val_loss: 0.2076 - val_accuracy: 0.9383\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 22s 448ms/step - loss: 0.0387 - accuracy: 0.9943 - val_loss: 0.1620 - val_accuracy: 0.9402\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 22s 454ms/step - loss: 0.0249 - accuracy: 0.9974 - val_loss: 0.1794 - val_accuracy: 0.9364\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 22s 455ms/step - loss: 0.0160 - accuracy: 0.9994 - val_loss: 0.1559 - val_accuracy: 0.9439\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 21s 440ms/step - loss: 0.0126 - accuracy: 0.9994 - val_loss: 0.1698 - val_accuracy: 0.9364\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 22s 442ms/step - loss: 0.0113 - accuracy: 0.9994 - val_loss: 0.1649 - val_accuracy: 0.9458\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 22s 451ms/step - loss: 0.0231 - accuracy: 0.9949 - val_loss: 0.1684 - val_accuracy: 0.9421\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_label = le.fit_transform(train_data['label'])\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,do_lower_case=True)\n",
    "train_text = train_data['main_tweet'].tolist()\n",
    "encoding = tokenizer(train_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "dev_data=preprocess_token(dev_data)\n",
    "dev_text = dev_data['main_tweet'].tolist()\n",
    "dev_encode = tokenizer(dev_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "dev_label = le.transform(dev_data['label'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TFAutoModel.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "print(encoding[\"input_ids\"].shape)\n",
    "\n",
    "def create_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(100,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(100,), name='attention_mask', dtype='int32')\n",
    "\n",
    "    embeddings = model(input_ids, attention_mask=mask)[0]\n",
    "#     X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\n",
    "    X = tf.keras.layers.LSTM(128)(embeddings)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "    X = tf.keras.layers.Dropout(0.1)(X)\n",
    "    y = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(X)\n",
    "\n",
    "    tfmodel = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "    return tfmodel\n",
    "\n",
    "tfmodel = create_model()\n",
    "optimizer = tf.keras.optimizers.Adam(2e-5)\n",
    "tfmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = tfmodel.fit({\"input_ids\": encoding[\"input_ids\"], 'attention_mask': encoding[\"attention_mask\"]}, \\\n",
    "                      train_label,\\\n",
    "                      validation_data=({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]}, dev_label),\\\n",
    "                      epochs=10)\n",
    "# tfmodel.load_weights('bert-weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "api_key = \"aJTgi4d1H1zmQNkQuHeualNhP\"\n",
    "api_secrets = \"yPFdp2Bbib25r1iPQCqBuPq8B9UzoJbIdpv1jgzEZFEg1eJl6X\"\n",
    "access_token = \"1409382627124019204-oPpzVGuCwyFfQTfoocFhgHi68whhog\"\n",
    "access_secret = \"esgwpH5gnRfNB0SpiHkO52mZSI5VKKnn8SG2pafEnzj2b\"\n",
    " \n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(api_key,api_secrets)\n",
    "auth.set_access_token(access_token,access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'E:\\\\Capstone\\\\Twitter-Rumour-Detection-main\\\\Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m                 tweets_dict[tweet\u001b[39m.\u001b[39mid_str][\u001b[39m'\u001b[39m\u001b[39mhashtag\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m hashtag[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m source_ids \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 21\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mE:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mCapstone\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mTwitter-Rumour-Detection-main\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mData\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     22\u001b[0m threads \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[0;32m     23\u001b[0m file\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'E:\\\\Capstone\\\\Twitter-Rumour-Detection-main\\\\Data'"
     ]
    }
   ],
   "source": [
    "def get_tweets_from_api(ids):\n",
    "    for i in range(0, len(ids), 100):\n",
    "        split = ids[i:i+100]\n",
    "        for tweet in api.lookup_statuses(split, tweet_mode='extended'):\n",
    "            tweets_dict[tweet.id_str] = {}\n",
    "            tweets_dict[tweet.id_str]['text'] = tweet.full_text\n",
    "            tweets_dict[tweet.id_str]['user'] = tweet.user.screen_name\n",
    "            tweets_dict[tweet.id_str]['verified'] = tweet.user.verified\n",
    "            tweets_dict[tweet.id_str]['followers'] = tweet.user.followers_count\n",
    "            tweets_dict[tweet.id_str]['num_statuses'] = tweet.user.statuses_count\n",
    "            tweets_dict[tweet.id_str]['likes'] = tweet.favorite_count\n",
    "            tweets_dict[tweet.id_str]['retweets'] = tweet.retweet_count\n",
    "            hashtag = tweet.entities['hashtags']\n",
    "            if hashtag == []:\n",
    "                tweets_dict[tweet.id_str]['hashtag'] = \"\"\n",
    "            else:\n",
    "                tweets_dict[tweet.id_str]['hashtag'] = hashtag[0]['text']\n",
    "\n",
    "                    \n",
    "source_ids = []\n",
    "file = open(r\"E:\\Capstone\\Twitter-Rumour-Detection-main\\Data\")\n",
    "threads = file.readlines()\n",
    "file.close()\n",
    "source_ids = [thread.replace('\\n', '').split(',')[0] for thread in threads]\n",
    "\n",
    "tweets_dict = {}\n",
    "\n",
    "get_tweets_from_api(source_ids)\n",
    "                      \n",
    "tweet_text = []\n",
    "tweet_user = []\n",
    "tweet_verified = []\n",
    "tweet_likes = []\n",
    "tweet_retweets = []\n",
    "tweet_hashtag = []\n",
    "tweet_follow = []\n",
    "tweet_statuses = []\n",
    "\n",
    "for tweet_id in tweets_dict.keys():\n",
    "    tweet_text.append(tweets_dict[tweet_id]['text'])\n",
    "    tweet_user.append(tweets_dict[tweet_id]['user'])\n",
    "    tweet_verified.append(tweets_dict[tweet_id]['verified'])\n",
    "    tweet_likes.append(tweets_dict[tweet_id]['likes'])\n",
    "    tweet_retweets.append(tweets_dict[tweet_id]['retweets'])\n",
    "    tweet_hashtag.append(tweets_dict[tweet_id]['hashtag'])\n",
    "    tweet_follow.append(tweets_dict[tweet_id]['followers'])\n",
    "    tweet_statuses.append(tweets_dict[tweet_id]['num_statuses'])\n",
    "    \n",
    "covid_ids_df = pd.DataFrame({'id': tweets_dict.keys(), \n",
    "                             'text': tweet_text, \n",
    "                             'user': tweet_user,\n",
    "                             'verified': tweet_verified,\n",
    "                             'likes': tweet_likes,\n",
    "                             'retweets': tweet_retweets,\n",
    "                             'hashtag': tweet_hashtag,\n",
    "                             'followers': tweet_follow,\n",
    "                             'statuses': tweet_statuses},\n",
    "                              index=tweets_dict.keys())\n",
    "\n",
    "covid_ids_df.to_csv('covid_id_df.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 142ms/step - loss: 0.1684 - accuracy: 0.9421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1684463620185852, 0.9420560598373413]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfmodel.save(\"modelbert.h5\")\n",
    "# dev_data=preprocess_token(dev_data)\n",
    "\n",
    "dev_text = dev_data['main_tweet'].tolist()\n",
    "dev_encode = tokenizer(dev_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "dev_label = le.transform(dev_data['label'])\n",
    "tfmodel.evaluate({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]}, dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert on dev set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9451    0.9833    0.9638       420\n",
      "           1     0.9286    0.7913    0.8545       115\n",
      "\n",
      "    accuracy                         0.9421       535\n",
      "   macro avg     0.9368    0.8873    0.9091       535\n",
      "weighted avg     0.9415    0.9421    0.9403       535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionzz = tfmodel.predict({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]})\n",
    "predictionzz = (predictionzz > 0.5).astype(\"int32\")\n",
    "predictionzz = np.ndarray.flatten(predictionzz)\n",
    "print(\"Bert on dev set\")\n",
    "print(classification_report(dev_label, predictionzz, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create Bert for test 2e5\n",
    "test_data=preprocess_token(test_data)\n",
    "\n",
    "test_text = test_data['main_tweet'].tolist()\n",
    "test_encode = tokenizer(test_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "prediction = tfmodel.predict({\"input_ids\": test_encode[\"input_ids\"], 'attention_mask': test_encode[\"attention_mask\"]})\n",
    "prediction = (prediction > 0.5).astype(\"int32\")\n",
    "prediction = np.ndarray.flatten(prediction)\n",
    "pd.DataFrame({\"Predicted\":  prediction}).to_csv('BertLSTM+Feedforward-submit.csv', index_label=\"Id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
